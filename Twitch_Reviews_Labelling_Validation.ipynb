{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d154f3f0",
   "metadata": {},
   "source": [
    "# Twitch-Review-Audit: Data Labeling Consistency & Noise Detection\n",
    "[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![Status: High Quality](https://img.shields.io/badge/Data_Quality-High-green.svg)](#)\n",
    "\n",
    "## Project Overview\n",
    "This project establishes a high-precision NLP pipeline to audit the data quality of Twitch mobile app reviews. In modern Machine Learning, **Data-Centric AI** focuses on the reliability of labels before model training. \n",
    "\n",
    "We implement a **Rule-Based Annotation** system to cross-validate user-provided ratings (Scores) against the semantic intent of their reviews.\n",
    "\n",
    "### üõ† Tech Stack\n",
    "* **NLP Pipeline:** NLTK (Tokenization, Regex Normalization)\n",
    "* **Metric:** Cohen‚Äôs Kappa Coefficient ($\\kappa$)\n",
    "* **Objective:** Label Noise Detection & Dataset Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a187984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and NLTK resources ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Marcell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "\n",
    "# Downloading necessary resource for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"Libraries imported and NLTK resources ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e36b4a",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition\n",
    "We load the raw Twitch review dataset. The primary features of interest are `content` (textual feedback) and `score` (numerical rating 1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff7808f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (97548, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get away with that UI. Uninstalled.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New layout is absolute rubbish. Dont try to be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please revert to the previous UI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolute hideous UI update since today, random...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UI is soooooo bad now</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score\n",
       "0                Get away with that UI. Uninstalled.      1\n",
       "1  New layout is absolute rubbish. Dont try to be...      1\n",
       "2                   Please revert to the previous UI      1\n",
       "3  Absolute hideous UI update since today, random...      1\n",
       "4                          The UI is soooooo bad now      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('twitch_reviews.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "# Displaying the first few rows for initial data inspection\n",
    "df[['content', 'score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289591e",
   "metadata": {},
   "source": [
    "## 2. Text Normalization & Tokenization\n",
    "To ensure the lexicon matches are precise, we apply a normalized pipeline. Since **Lemmatization is omitted** to preserve the raw intensity of gaming slang, we prioritize clean tokenization.\n",
    "\n",
    "**Strategy:**\n",
    "1. **Case Folding:** Lowercasing all text.\n",
    "2. **Noise Removal:** Eliminating URLs and non-alphabetic characters using Regex.\n",
    "3. **Word Segmentation:** Utilizing `nltk.word_tokenize` for discrete token mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ce26c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete: Text normalized and tokenized.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get away with that UI. Uninstalled.</td>\n",
       "      <td>[get, away, with, that, ui, uninstalled]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New layout is absolute rubbish. Dont try to be...</td>\n",
       "      <td>[new, layout, is, absolute, rubbish, dont, try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please revert to the previous UI</td>\n",
       "      <td>[please, revert, to, the, previous, ui]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolute hideous UI update since today, random...</td>\n",
       "      <td>[absolute, hideous, ui, update, since, today, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UI is soooooo bad now</td>\n",
       "      <td>[the, ui, is, soooooo, bad, now]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0                Get away with that UI. Uninstalled.   \n",
       "1  New layout is absolute rubbish. Dont try to be...   \n",
       "2                   Please revert to the previous UI   \n",
       "3  Absolute hideous UI update since today, random...   \n",
       "4                          The UI is soooooo bad now   \n",
       "\n",
       "                                              tokens  \n",
       "0           [get, away, with, that, ui, uninstalled]  \n",
       "1  [new, layout, is, absolute, rubbish, dont, try...  \n",
       "2            [please, revert, to, the, previous, ui]  \n",
       "3  [absolute, hideous, ui, update, since, today, ...  \n",
       "4                   [the, ui, is, soooooo, bad, now]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove URLs, hashtags, and mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove special characters and numbers, keeping only alphabets\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['content'].apply(clean_and_tokenize)\n",
    "print(\"Preprocessing complete: Text normalized and tokenized.\")\n",
    "df[['content', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f947a29",
   "metadata": {},
   "source": [
    "## 3. Rule-Based Annotation (Heuristic Model)\n",
    "\n",
    "We employ a **curated rule-based lexical heuristic** tailored specifically to the Twitch ecosystem. The lexicon incorporates domain-specific expressions and accounts for common morphological variations (e.g., *lag, lagging, lags*) to maintain interpretability while compensating for the absence of lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a1f3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon-based labeling complete.\n"
     ]
    }
   ],
   "source": [
    "# Expanded Lexicon curated for Twitch-specific feedback\n",
    "pos_lexicon = [\n",
    "    'awesome', 'love', 'loved', 'perfect', 'best', 'good', 'nice', 'fun', \n",
    "    'cool', 'great', 'amazing', 'excellent', 'smooth', 'helpful', 'pog', 'poggers'\n",
    "]\n",
    "\n",
    "neg_lexicon = [\n",
    "    'worst', 'garbage', 'trash', 'rubbish', 'horrible', 'hideous', 'hate', 'hated',\n",
    "    'terrible', 'uninstall', 'uninstalled', 'revert', 'reverted', 'previous', 'old',\n",
    "    'broken', 'lag', 'lagging', 'lags', 'slow', 'slows', 'buggy', 'bugs', \n",
    "    'tiktok', 'bad', 'mess', 'cluttered', 'ads', 'advertise', 'advertising'\n",
    "]\n",
    "\n",
    "def rule_based_label(tokens):\n",
    "    \"\"\"\n",
    "    Annotator 1 Logic:\n",
    "    Calculates sentiment based on exact matches within the token list.\n",
    "    \"\"\"\n",
    "    pos_count = sum(1 for word in tokens if word in pos_lexicon)\n",
    "    neg_count = sum(1 for word in tokens if word in neg_lexicon)\n",
    "    \n",
    "    if neg_count > pos_count:\n",
    "        return 0  # Negative\n",
    "    elif pos_count > neg_count:\n",
    "        return 1  # Positive\n",
    "    else:\n",
    "        return 2  # Neutral / Ambiguous\n",
    "\n",
    "df['label_lexicon'] = df['tokens'].apply(rule_based_label)\n",
    "print(\"Lexicon-based labeling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d84d2",
   "metadata": {},
   "source": [
    "## 4. Agreement Analysis Between Labeling Sources\n",
    "\n",
    "The **original user-provided rating** and the **rule-based lexical heuristic output** are treated as two independent labeling sources. To evaluate their consistency, we compute the following metrics:\n",
    "\n",
    "1. **Agreement Rate (Accuracy):** The proportion of samples where both labeling sources assign the same sentiment label.\n",
    "2. **Cohen‚Äôs Kappa ($\\kappa$):** A chance-corrected agreement statistic that measures the level of consistency beyond random agreement.\n",
    "\n",
    "$$\n",
    "\\kappa = \\frac{p_o - p_e}{1 - p_e}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a43e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_score_to_label(score):\n",
    "    if score <= 2:\n",
    "        return 0  # Negative\n",
    "    elif score >= 4:\n",
    "        return 1  # Positive\n",
    "    else:\n",
    "        return 2  # Neutral\n",
    "\n",
    "df['label_original'] = df['score'].apply(map_score_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecf76b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUALITY AUDIT RESULTS ===\n",
      "Total Evaluated Samples: 55989\n",
      "Agreement Rate (Accuracy): 94.90%\n",
      "Cohen's Kappa Score: 0.8211\n",
      "Interpretation: Substantial Agreement - Lexicon rules are highly consistent.\n"
     ]
    }
   ],
   "source": [
    "# Filtering out 'Neutral' samples to focus on clear Sentiment Agreement\n",
    "df_eval = df[(df['label_lexicon'] != 2) & (df['label_original'] != 2)].copy()\n",
    "\n",
    "accuracy = accuracy_score(df_eval['label_original'], df_eval['label_lexicon'])\n",
    "kappa = cohen_kappa_score(df_eval['label_original'], df_eval['label_lexicon'])\n",
    "\n",
    "print(\"=== QUALITY AUDIT RESULTS ===\")\n",
    "print(f\"Total Evaluated Samples: {len(df_eval)}\")\n",
    "print(f\"Agreement Rate (Accuracy): {accuracy:.2%}\")\n",
    "print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n",
    "\n",
    "# Final Interpretation\n",
    "if kappa > 0.60:\n",
    "    print(\"Interpretation: Substantial Agreement - Lexicon rules are highly consistent.\")\n",
    "elif kappa > 0.40:\n",
    "    print(\"Interpretation: Moderate Agreement - Room for lexicon expansion.\")\n",
    "else:\n",
    "    print(\"Interpretation: Low Agreement - High risk of sarcasm or misaligned ratings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12e1e9",
   "metadata": {},
   "source": [
    "## 5. Label Inconsistency Detection & Remediation\n",
    "\n",
    "We identified **4,909 samples (8.7%)** exhibiting **potential label inconsistency** between user-provided ratings and textual sentiment inferred by the rule-based lexical heuristic.\n",
    "\n",
    "**Operational Definition:**  \n",
    "Samples in which the numerical rating (e.g., 5 stars) diverges from the dominant linguistic sentiment expressed in the review text (e.g., explicit negative expressions such as ‚Äúthis app is rubbish‚Äù).\n",
    "\n",
    "**Recommended Actions for AI Training:**\n",
    "- Flag these samples for **manual review or re-annotation** within a Human-in-the-Loop (HITL) framework.\n",
    "- Excluding or correcting highly inconsistent samples may reduce noisy supervision and improve the robustness of downstream sentiment classification models (e.g., BERT or SVM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f893ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Label Noise: 4909 samples.\n",
      "\n",
      "Top samples requiring manual re-annotation:\n"
     ]
    }
   ],
   "source": [
    "# Identifying 'Label Noise'\n",
    "df['is_noise'] = (df['label_original'] != df['label_lexicon']) & (df['label_lexicon'] != 2)\n",
    "noise_data = df[df['is_noise'] == True]\n",
    "\n",
    "print(f\"Detected Label Noise: {len(noise_data)} samples.\")\n",
    "print(\"\\nTop samples requiring manual re-annotation:\")\n",
    "noise_data[['content', 'score', 'label_lexicon']].head(10)\n",
    "\n",
    "# Saving the audited dataset for further review\n",
    "df.to_csv('twitch_reviews_audited.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fa2b2",
   "metadata": {},
   "source": [
    "## üèÅ 6. Final Conclusions & Strategic Recommendations\n",
    "\n",
    "### 6.1 Key Performance Insights\n",
    "Based on the quality audit of **55,989 samples**, the following conclusions were drawn:\n",
    "\n",
    "* **Strong Heuristic Consistency:** A **Cohen‚Äôs Kappa score of 0.8211** indicates **substantial to near-perfect agreement** between the rule-based lexical heuristic and user-provided ratings. This suggests strong alignment under clearly polarized sentiment conditions and indicates that the selected domain-specific keywords (e.g., *‚Äúpog‚Äù*, *‚Äúrevert‚Äù*, *‚Äútiktok‚Äù*) capture dominant sentiment signals commonly expressed in Twitch reviews.\n",
    "* **High Label Consistency:** The observed **Agreement Rate of 94.90%** reflects a high level of consistency between numerical ratings and textual feedback, which is notable for large-scale social media review datasets.\n",
    "* **Effective Inconsistency Identification:** The pipeline identified **4,909 samples (8.7%)** exhibiting **potential label inconsistency**, where the sentiment expressed in text diverges from the assigned star rating. These cases often correspond to sarcasm, mixed sentiment, or rating bias.\n",
    "\n",
    "### 6.2 Impact on AI Training\n",
    "Identifying potentially inconsistent samples is critical for improving downstream machine learning workflows:\n",
    "1. **Reduced Noisy Supervision:** Excluding or correcting highly inconsistent samples may prevent models from learning misleading or spurious correlations.\n",
    "2. **Improved Model Robustness:** Training on higher-consistency data can help models better distinguish dominant patterns in complaints versus praise.\n",
    "\n",
    "### 6.3 Future Recommendations\n",
    "To further enhance this NLP pipeline, the following steps are recommended:\n",
    "* **N-gram Analysis:** Incorporate **bigrams** to better capture negation and contextual polarity (e.g., distinguishing *‚Äúnot good‚Äù* from *‚Äúgood‚Äù*).\n",
    "* **Human-in-the-Loop Validation:** Implement a **Human-in-the-Loop (HITL)** process where samples flagged as potentially inconsistent are manually reviewed or re-annotated.\n",
    "* **Model Benchmarking:** Following dataset auditing, future work may involve training and benchmarking **transformer-based models (e.g., BERT)** on the high-consistency subset to evaluate potential performance improvements over traditional approaches.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
